Storage Volumes for Workflow Runs
=================================

The serial workflow engine supports the use of different workers to execute individual workflow steps. Each worker is associated with a storage volume to ensure that the worker has access to all files that are used for execution and that output files can be collected for following workflow steps and the final workflow run result.


Volume Manager and Storage Volume
---------------------------------

The **volume manager** maintains the list of files that are available in the run environment of the workflow. At the beginning of the workflow run, these files are the static input files that are part of the workflow template. During workflow execution, output files that are generated by individual workflow steps are added to the list of available files. For each file a list of storage volumes that contain the latest version of the file is maintained. Overriding static files should be prohibited although it is possible for workflow steps that operate on the original files in the file system that maintains the static files.

The volume manager maintains a set of **storage volumes**. Each volume has a unique name.  **flowServ** supports different types of storage volumes, including the local file system, remote file systems accessible via ssh, and cloud storage buckets. With each storage volume the volume manager maintains the files that are available on the storage manager.

A default storage volume is created for each workflow. This volume is associated with the local run directory that is created for the workflow run.


Storage Volumes and Workers
---------------------------

Each worker is associated with one or more storage volumes. When a worker is selected to execute a workflow step, the volume manager ensures that all the required files in their latest version are available on at least one of the associated storage volumes. Output files are only downloaded from a storage volume if they are needed for another workflow step or if they are defined as part of the final workflow output.


It is important to note that a worker has to be associated with storage volumes that are suitable for it. The type of the worker does not directly determine the type of the storage volumes. In addition, multiple workers can share the same volume. Thus, there isn't a 1:1 mapping between worker instances and storage volumes. For example, the local sub-process worker and the Docker worker both use a local file system storage volume. Workers for cloud services, on the other hand, may work with any bucket-type or Internet-accessible storage volume.

It is the responsibility of the user to ensure that all workers are associated with appropriate storage volumes. If no volume is specified for a worker the default (local file system) storage volume is used.


Configuration
-------------

Storage volumes for a workflow that are instantiated in addition to the default storage volume are defined as a list of objects where each object contains the following fields:

.. code-block:: yaml

    volumes:
        - name: str
          type: str
          config: dict


The name is a unique volume identifier and the type determines the class of the storage volume that is being instantiated. The implementation-specific configuration object will be passed on the the instantiated class as keyword arguments. Note that the name ``__default__`` is reserved for the default storage volume. The user can provide a configuration for the default store. The list of volume configurations is part of the engine configuration object. We may want to have these configurations at the level of the workflow and not the workflow engine. The engine may define a set of defaults that can be overwritten by the workflow configuration.
